{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Differentiation: From symbolic differentiation to automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbolic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define symbolic variables\n",
    "x = sp.Symbol('x')\n",
    "\n",
    "# Define a function\n",
    "f = sp.sin(x) + x**2\n",
    "\n",
    "# Compute derivatives\n",
    "df_dx = sp.diff(f, x)  # Partial derivative with respect to x\n",
    "\n",
    "# Print the result\n",
    "print(df_dx)\n",
    "# Output: 2*x + cos(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dx.subs({\"x\": 1}).evalf()  # Evaluate the derivative at x=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "def f(x):\n",
    "    return x**2 + np.sin(x)\n",
    "\n",
    "optimize.approx_fprime([1.0], f, 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax import grad, jit, vmap\n",
    "\n",
    "def f(x):\n",
    "    return jnp.sin(x) + x**2\n",
    "\n",
    "f_p = grad(f)\n",
    "print(f_p(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It supports higher order derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_pp = grad(grad(f))\n",
    "print(f_pp(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the function and its derivatives.\n",
    "\n",
    "Note: We'll use `vmap` to vectorize the function and its derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.linspace(-2, 2, 100)\n",
    "y = vmap(f)(x)\n",
    "y_p = vmap(f_p)(x)\n",
    "y_pp = vmap(f_pp)(x)\n",
    "\n",
    "plt.plot(x, y, label='f(x)')\n",
    "plt.plot(x, y_p, label=\"f'(x)\")\n",
    "plt.plot(x, y_pp, label=\"f''(x)\")\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do it with `torch.func`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import func, Tensor\n",
    "import torch\n",
    "\n",
    "def f(x: Tensor) -> Tensor:\n",
    "    return x**2 + x.sin()\n",
    "\n",
    "x = torch.ones([])\n",
    "grad_f = func.grad(f)\n",
    "grad_f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more ML-oriented example with a multi-layer perceptron using `tanh` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, inputs):\n",
    "    outputs = inputs\n",
    "    for W, b in params:\n",
    "        outputs = jnp.dot(inputs, W) + b\n",
    "        outputs = jnp.tanh(outputs)\n",
    "    return outputs\n",
    "\n",
    "key = jax.random.key(0)\n",
    "inputs = jax.random.normal(key, shape=(100, 10))\n",
    "targets = jax.random.normal(key, shape=(100, 1))\n",
    "n_layers = 3\n",
    "params = [\n",
    "    (jax.random.normal(key, shape=(10, 10)), jnp.zeros(10)) for _ in range(n_layers)\n",
    "]\n",
    "outputs = predict(params, inputs)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is the mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fun(params, inputs, targets):\n",
    "    preds = predict(params, inputs)\n",
    "    return jnp.sum((preds - targets)**2)\n",
    "\n",
    "loss_fun(params, inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Could I compute the gradient of the `predict` function?\n",
    "\n",
    "Eg using:\n",
    "\n",
    "```python\n",
    "grad_fn = grad(predict)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad(predict)(params, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_fun = jit(grad(loss_fun, argnums=0))\n",
    "\n",
    "grad_params = grad_fun(params, inputs, targets)\n",
    "len(grad_params[0]), grad_params[0][0].shape, grad_params[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobian with forward and reverse mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.testing import assert_allclose\n",
    "from jax import jacfwd, jacrev\n",
    "\n",
    "W = jax.random.normal(key, (3, 3))\n",
    "\n",
    "def f(x):\n",
    "    return jnp.dot(W, x)\n",
    "\n",
    "x = jax.random.normal(key, (3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_f = jacfwd(f)\n",
    "assert_allclose(J_f(x), W)\n",
    "J_f(x) - W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_f = jacrev(f)\n",
    "assert_allclose(J_f(x), W)\n",
    "J_f(x) - W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic(x):\n",
    "    return 0.5 * x.T @ W @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_quadratic = grad(quadratic)\n",
    "assert_allclose(grad_quadratic(x), W @ x)  # FAILS WHY?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = jacfwd(jacrev(quadratic))  # Hessian via forward-over-reverse\n",
    "assert_allclose(H(x), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A full example with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X = X[:, :2]  # Use only the first feature to be able to visualize the decision boundary\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "y[y == 0] = -1  # Convert y to -1 and 1\n",
    "\n",
    "X = jnp.array(X)\n",
    "y = jnp.array(y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params, X, y):\n",
    "    w, b = params\n",
    "    logits = X @ w + b\n",
    "    return jnp.mean(jnp.log(1 + jnp.exp(- y * logits)))\n",
    "\n",
    "w = jnp.zeros(X.shape[1])\n",
    "b = jnp.zeros(1)\n",
    "\n",
    "params = (w, b)\n",
    "\n",
    "grad_fn = jit(grad(loss_fn))\n",
    "\n",
    "# Gradient descent\n",
    "lr = 0.05\n",
    "for _ in range(10_000):\n",
    "    grad_params = grad_fn(params, X, y)\n",
    "    params = [p - lr * g for p, g in zip(params, grad_params)]\n",
    "    accuracy = jnp.mean(jnp.sign(X @ params[0] + params[1]) == y)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "xx = jnp.linspace(4, 8, 100)\n",
    "yy = (-params[1] - params[0][0] * xx) / params[0][1]\n",
    "plt.plot(xx, yy, 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exercise1 : Implement the Newton's method for optimization.\n",
    "- Exercise2 : Implement the multi-class logistic regression (using softmax) with the 3 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctrldev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
