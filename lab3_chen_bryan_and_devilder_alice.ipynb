{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Lab 3 : Proximal/cyclic/greedy coordinate descent\n",
    "\n",
    "#### Authors: A. Gramfort, H. Janati, M. Massias\n",
    "\n",
    "## Aim\n",
    "\n",
    "The aim of this material is to code \n",
    "- cyclic and greedy coordinate descent for ordinary least squares (OLS)\n",
    "- proximal coordinate descent for sparse Logistic regression\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work **before the 22nd of november at noon**, using the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**\n",
    "- The **name of the file must be** constructed as in the next cell\n",
    "\n",
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab3_chen_bryan_and_devilder_alice.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"bryan\"\n",
    "ln1 = \"chen\"\n",
    "fn2 = \"alice\"\n",
    "ln2 = \"devilder\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"lab3\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alice\\AppData\\Local\\Temp\\ipykernel_26332\\428856815.py:4: DeprecationWarning: Please use `toeplitz` from the `scipy.linalg` namespace, the `scipy.linalg.special_matrices` namespace is deprecated.\n",
      "  from scipy.linalg.special_matrices import toeplitz\n"
     ]
    }
   ],
   "source": [
    "# the usual functions:\n",
    "\n",
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "def simu(coefs, n_samples=1000, corr=0.5, for_logreg=False):\n",
    "    n_features = len(coefs)\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    A = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    b = A.dot(coefs) + randn(n_samples)\n",
    "    if for_logreg:\n",
    "        b = np.sign(b)\n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Ordinary Least Squares\n",
    "\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{n \\times p}$, $y \\in \\mathbb{R}^n$.\n",
    "We want to use coordinate descent to solve:\n",
    "    $$\\hat w \\in  \\mathrm{arg \\, min \\,} \\frac 12 \\Vert Aw - b \\Vert ^2 $$\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 1:</b> We ask you to code\n",
    "     <ul>\n",
    "         <li>cyclic coordinate descent: at iteration $t$, update feature $j = t \\mod p$</li>\n",
    "         <li>greedy coordinate descent: at iteration $t$, update feature having the largest partial gradient in magnitude, ie $j = \\mathrm{arg\\, max \\,}_{i} \\vert \\nabla_i f(w_t) \\vert$.\n",
    "</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**WARNING**: You must do this in a clever way, ie such that $p$ updates cost the same as one update of GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 100\n",
    "np.random.seed(1970)\n",
    "coefs = np.random.randn(n_features)\n",
    "\n",
    "A, b = simu(coefs, n_samples=1000, for_logreg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_cd(A, b, n_iter):\n",
    "    n_samples, n_features = A.shape\n",
    "    all_objs = []\n",
    "    \n",
    "    w = np.zeros(n_features)\n",
    "    residuals = b - A.dot(w)\n",
    "    \n",
    "    # TODO\n",
    "    lips_const = np.linalg.norm(A, axis=0) ** 2\n",
    "    # END TODO\n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        j = t % n_features\n",
    "        # TODO\n",
    "        old_w_j = w[j]\n",
    "        w[j] += A[:, j].dot(residuals) / lips_const[j]\n",
    "        # update residuals:\n",
    "        residuals += A[:, j] * (old_w_j - w[j])\n",
    "        # END TODO\n",
    "        \n",
    "        if t % n_features == 0:\n",
    "            all_objs.append((residuals ** 2).sum() / 2.)\n",
    "    return w, np.array(all_objs)\n",
    "\n",
    "\n",
    "\n",
    "def greedy_cd(A, b, n_iter):\n",
    "    n_samples, n_features = A.shape\n",
    "    all_objs = []\n",
    "    \n",
    "    w = np.zeros(n_features)\n",
    "    \n",
    "    gradient = A.T.dot(A.dot(w) - b)\n",
    "    gram = A.T.dot(A)  # you will need this to keep the gradient up to date\n",
    "    \n",
    "    # TODO\n",
    "    lips_const = np.linalg.norm(A, axis=0) ** 2\n",
    "    # END TODO \n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        # TODO\n",
    "        # choose feature j to update: \n",
    "        j = np.argmax(np.abs(gradient))\n",
    "        old_w_j = w[j]\n",
    "        w[j] -= gradient[j] / lips_const[j]\n",
    "        # update gradient:\n",
    "        gradient += gram[:, j] * (w[j] - old_w_j)\n",
    "        # END TODO\n",
    "        \n",
    "        if t % n_features == 0:\n",
    "            all_objs.append(0.5 * np.linalg.norm(A.dot(w) - b) ** 2)\n",
    "    \n",
    "    return w, np.array(all_objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 2:</b>\n",
    "     <ul>\n",
    "         <li>Compute a precise minimum with your favorite solver</li>\n",
    "         <li>Compare the performance of cyclic and greedy CD as function of iterations.</li>\n",
    "         <li>From a practical point of view, could you use greedy CD for L2 regularized logistic regression? to solve OLS, but with 100,000 features? Explain your answers.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**Remark:** You will do the plots using the number of iterations on the x-axis and not time as your code is likely to be slow unless you use [numba](https://numba.pydata.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_linreg(w):\n",
    "    \"\"\"Least-squares loss\"\"\"\n",
    "    return np.linalg.norm(A @ w - b, ord=2)**2/2\n",
    "\n",
    "def grad_linreg(w):\n",
    "    \"\"\"Leas-squares gradient\"\"\"\n",
    "    return A.T @ (A @ w - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compute a precise minimum with your favorite solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precise minimum with favorite solver (lstsq) = 466.3849765337658 \n",
      "\n",
      "Gradient norm at w_min = 5.133562688586661e-11\n"
     ]
    }
   ],
   "source": [
    "# compute precise minimum with your favorite solver\n",
    "\n",
    "from numpy.linalg import lstsq\n",
    "\n",
    "w_min, _, _, _ = lstsq(A, b, rcond=None)\n",
    "min_lstsq = loss_linreg(w_min)\n",
    "\n",
    "# print(w_min)\n",
    "print(f\"Precise minimum with favorite solver (lstsq) = {min_lstsq} \\n\")\n",
    "print(f\"Gradient norm at w_min = {norm(grad_linreg(w_min))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compare the performance of cyclic and greedy CD as function of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of cyclic coordinate descent = 0.025 seconds \n",
      "\n",
      "Duration of greedy coordinate descent = 0.037 seconds \n",
      "\n",
      "Precise minimum with cyclic coordinate descent = 466.3849765339347 \n",
      "Gradient norm at w_cyclic = 0.00030884372297994647 \n",
      "\n",
      "Precise minimum with greedy coordinate descent = 466.384976533766\n",
      "Gradient norm at w_greedy = 1.0335130410048501e-05\n"
     ]
    }
   ],
   "source": [
    "n_iter = 3000\n",
    "\n",
    "start_time = time.time()\n",
    "w_cyclic, all_objs_cyclic = cyclic_cd(A, b, n_iter)\n",
    "print(f\"Duration of cyclic coordinate descent = {time.time() - start_time:.3f} seconds \\n\")\n",
    "start_time = time.time()\n",
    "w_greedy, all_objs_greedy = greedy_cd(A, b, n_iter)\n",
    "print(f\"Duration of greedy coordinate descent = {time.time() - start_time:.3f} seconds \\n\")\n",
    "\n",
    "print(f\"Precise minimum with cyclic coordinate descent = {loss_linreg(w_cyclic)} \")\n",
    "print(f\"Gradient norm at w_cyclic = {norm(grad_linreg(w_cyclic))} \\n\")\n",
    "print(f\"Precise minimum with greedy coordinate descent = {loss_linreg(w_greedy)}\")\n",
    "print(f\"Gradient norm at w_greedy = {norm(grad_linreg(w_greedy))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small remark: We notice that the greedy coordinate descent gives a slightly better solution than the cyclic one because the gradient norm is smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466.3849765337658\n"
     ]
    }
   ],
   "source": [
    "print(min_lstsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_opt_min = [min_lstsq for i in range(min(len(all_objs_greedy), 100))]\n",
    "\n",
    "plt.plot(all_objs_cyclic, label='Cyclic CD')\n",
    "plt.plot(all_objs_greedy, label='Greedy CD')\n",
    "plt.plot(list_opt_min, label='Optimal objective', alpha=0.4)\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Objective')\n",
    "plt.title('Convergence plot for Cyclic and Greedy CD')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "\n",
    "We can observe that the Greedy Coordinate Descent converges faster than the Cyclic Coordinate Descent. This is due to the fact that the Greedy CD selects the coordinate that has the largest gradient in magnitude, which allows it to converge faster. The Greedy CD reaches the minimum in 5 iterations, while the Cyclic CD reaches it in 7 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. From a practical point of view, could you use greedy CD for L2 regularized logistic regression? to solve OLS, but with 100,000 features? Explain your answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Greedy CD for L2-regularized logistic regression:**\n",
    "\n",
    "From a practical standpoint, using greedy coordinate descent (CD) for L2-regularized logistic regression is not ideal. Greedy CD involves selecting and updating the coordinate that provides the greatest decrease in the objective function at each iteration. While coordinate descent can be effective for problems with certain structures, such as L1 regularization where updates can be computed efficiently, it becomes less practical for L2-regularized logistic regression with a large number of features.\n",
    "\n",
    "In L2-regularized logistic regression, the objective function is smooth but not separable across coordinates due to the logistic loss. Each coordinate update requires computing gradients that depend on all data points, which can be computationally expensive. With 100,000 features, the cost per iteration becomes significant, and the number of iterations required to converge can be large. Moreover, greedy selection adds overhead because it requires evaluating the potential improvement for each coordinate, increasing computational complexity.\n",
    "\n",
    "* **Greedy CD for OLS with 100,000 features:**\n",
    "\n",
    "Similarly, using coordinate descent for Ordinary Least Squares (OLS) with 100,000 features is not practical. OLS has a quadratic loss function, and while coordinate descent can be applied, each coordinate update involves computing the residuals and updating them, which can be costly when the number of features is large. The benefit of coordinate descent diminishes in high-dimensional settings because the per-iteration cost and the total number of iterations increase.\n",
    "\n",
    "In practice, for large-scale problems with many features, optimization methods like stochastic gradient descent (SGD) or mini-batch gradient descent are more suitable. These methods scale better with the number of features and data points, and they can exploit the smoothness and convexity of the objective function more effectively than coordinate descent in such settings.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "From a practical perspective, using greedy coordinate descent for L2-regularized logistic regression or OLS with 100,000 features is not advisable due to computational inefficiency. The high dimensionality makes coordinate updates expensive and the overhead of greedy selection substantial, making other optimization methods more suitable for such large-scale problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Sparse Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An important result\n",
    "\n",
    "Remember: we are solving \n",
    "$$\\hat w \\in \\mathrm{arg \\, min} \\sum_{i=1}^{n} \\mathrm{log} ( 1 + e^{- y_i w^\\top x_i} )  + \\lambda \\Vert w \\Vert_1$$\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 3:</b><br/>\n",
    "    Assuming uniqueness of the solution, show that: \n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\lambda \\geq \\lambda_{max} \\Leftrightarrow \\hat w = 0\n",
    ", \\text{where }\\lambda_{max} := \\frac 12 \\Vert X^\\top y \\Vert_{\\infty}\n",
    "$$\n",
    "\n",
    "**HINT:** You will need the following beautiful result: for any $w =(w_1, \\dots, w_p) \\in \\mathbb{R}^p$, the subdifferential of the L1 norm at $w$ is:\n",
    "\n",
    "$$\n",
    "\\partial \\Vert \\cdot \\Vert_1 (w) = \\partial \\vert \\cdot \\vert (w_1)  \\times \\dots \\times \\partial \\vert \\cdot \\vert (w_p) \n",
    "$$\n",
    "\n",
    "where $\\times$ is the Cartesian product between sets,\n",
    "and \n",
    "$$ \n",
    "\\partial \\vert \\cdot \\vert (w_j) = \n",
    "\\begin{cases} &w_j / |w_j| &\\mathrm{if} \\quad w_j \\neq 0, \n",
    "         \\\\ & [-1, 1] &\\mathrm{otherwise.} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "(it should now be easy to find $\\partial \\Vert \\cdot \\Vert_1 (\\mathbf{0}_p)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "To show that $\\hat{w} = 0_p$ is the unique minimizer if and only if $\\lambda \\geq \\lambda_{\\max} := \\frac{1}{2}\\|X^\\top y\\|_\\infty$, we use subgradient optimality conditions for convex functions.\n",
    "\n",
    "Consider the optimization problem:\n",
    "\n",
    "$$\n",
    "\\hat{w} \\in \\operatorname*{arg\\,min}_{w \\in \\mathbb{R}^p} \\left[ f(w) + \\lambda \\|w\\|_1 \\right],\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "f(w) = \\sum_{i=1}^{n} \\log\\left(1 + e^{-y_i w^\\top x_i}\\right).\n",
    "$$\n",
    "\n",
    "The gradient of $f(w)$ with respect to $w$ is:\n",
    "\n",
    "$$\n",
    "\\nabla f(w) = -\\sum_{i=1}^{n} y_i x_i \\frac{e^{-y_i w^\\top x_i}}{1 + e^{-y_i w^\\top x_i}} = -\\sum_{i=1}^{n} y_i x_i \\sigma(-y_i w^\\top x_i),\n",
    "$$\n",
    "\n",
    "where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function.\n",
    "\n",
    "At $w = 0_p$, since $\\sigma(0) = \\frac{1}{2}$, we have:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla f(0_p) &= -\\sum_{i=1}^{n} y_i x_i \\left( \\frac{1}{2} \\right) \\\\\n",
    "&= -\\frac{1}{2} X^\\top y.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The subdifferential of the $L_1$-norm at $w = 0_p$ is:\n",
    "\n",
    "$$\n",
    "\\partial \\|w\\|_1 \\big|_{w=0_p} = [-1, 1]^p.\n",
    "$$\n",
    "\n",
    "The necessary and sufficient condition for $w = 0_p$ to be a minimizer is:\n",
    "\n",
    "$$\n",
    "0 \\in \\nabla f(0_p) + \\lambda \\, \\partial \\|w\\|_1 \\big|_{w=0_p}.\n",
    "$$\n",
    "\n",
    "Substituting the computed gradient and subdifferential, we get:\n",
    "\n",
    "$$\n",
    "0 \\in -\\frac{1}{2} X^\\top y + \\lambda s, \\quad \\text{where } s \\in [-1, 1]^p.\n",
    "$$\n",
    "\n",
    "This implies:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} X^\\top y \\in \\lambda s, \\quad s \\in [-1, 1]^p.\n",
    "$$\n",
    "\n",
    "Therefore, for each component $j$:\n",
    "\n",
    "$$\n",
    "\\left| \\frac{1}{2} (X^\\top y)_j \\right| \\leq \\lambda.\n",
    "$$\n",
    "\n",
    "This leads to the condition:\n",
    "\n",
    "$$\n",
    "\\lambda \\geq \\frac{1}{2} \\| X^\\top y \\|_\\infty = \\lambda_{\\max}.\n",
    "$$\n",
    "\n",
    "Hence, if $\\lambda \\geq \\lambda_{\\max}$, then $w = 0_p$ satisfies the optimality condition and is the unique minimizer. Conversely, if $\\hat{w} = 0_p$ is the unique minimizer, the optimality condition must hold, implying $\\lambda \\geq \\lambda_{\\max}$.\n",
    "\n",
    "Therefore, we conclude that:\n",
    "\n",
    "$$\n",
    "\\lambda \\geq \\lambda_{\\max} \\quad \\Leftrightarrow \\quad \\hat{w} = 0_p.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 4:</b><br/>\n",
    "    Show that for sparse Logistic regression the coordinate-wise Lipschitz constant of the smooth term, $\\gamma_j$, can be taken equal to $\\Vert X_j \\Vert^2 / 4$, where $X_j$ denotes the $j$-th column of $X$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "To show that for sparse logistic regression, the coordinate-wise Lipschitz constant of the smooth term $\\gamma_j$ can be taken equal to $\\| X_j \\|^2 / 4$, where $X_j$ denotes the $j$-th column of $X$, we proceed as follows.\n",
    "\n",
    "The smooth part of the logistic regression objective function is:\n",
    "\n",
    "$$\n",
    "f(w) = \\sum_{i=1}^{n} \\log\\left(1 + e^{- y_i w^\\top x_i}\\right).\n",
    "$$\n",
    "\n",
    "We are interested in finding the Lipschitz constant $\\gamma_j$ for the gradient component $\\frac{\\partial f}{\\partial w_j}(w)$ with respect to $w_j$. Specifically, we need to find $\\gamma_j$ such that for all $w, w'$:\n",
    "\n",
    "$$\n",
    "\\left| \\frac{\\partial f}{\\partial w_j}(w) - \\frac{\\partial f}{\\partial w_j}(w') \\right| \\leq \\gamma_j \\left| w_j - w_j' \\right|.\n",
    "$$\n",
    "\n",
    "First, we compute the partial derivative of $f(w)$ with respect to $w_j$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial w_j}(w) = \\sum_{i=1}^{n} -y_i x_{ij} \\sigma(- y_i w^\\top x_i),\n",
    "$$\n",
    "\n",
    "where $\\sigma(t) = \\frac{1}{1 + e^{-t}}$ is the sigmoid function, and $x_{ij}$ is the $j$-th element of the vector $x_i$.\n",
    "\n",
    "Next, we compute the second derivative with respect to $w_j$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial w_j^2}(w) = \\sum_{i=1}^{n} x_{ij}^2 \\sigma(- y_i w^\\top x_i) \\left(1 - \\sigma(- y_i w^\\top x_i)\\right).\n",
    "$$\n",
    "\n",
    "Note that for any real number $t$, the product $\\sigma(t)(1 - \\sigma(t))$ attains its maximum value of $\\frac{1}{4}$ when $t = 0$, and is always non-negative and less than or equal to $\\frac{1}{4}$:\n",
    "\n",
    "$$\n",
    "0 \\leq \\sigma(t)(1 - \\sigma(t)) \\leq \\frac{1}{4}.\n",
    "$$\n",
    "\n",
    "Therefore, we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 f}{\\partial w_j^2}(w) \\leq \\frac{1}{4} \\sum_{i=1}^{n} x_{ij}^2 = \\frac{1}{4} \\| X_j \\|^2,\n",
    "$$\n",
    "\n",
    "where $\\| X_j \\|^2 = \\sum_{i=1}^{n} x_{ij}^2$ is the squared Euclidean norm of the $j$-th column of $X$.\n",
    "\n",
    "Since the second derivative provides an upper bound on the rate of change of the gradient, the Lipschitz constant $\\gamma_j$ for the gradient component $\\frac{\\partial f}{\\partial w_j}(w)$ can be taken as $\\gamma_j = \\frac{1}{4} \\| X_j \\|^2$.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "For sparse logistic regression, the coordinate-wise Lipschitz constant of the smooth term $\\gamma_j$ can be taken equal to $\\| X_j \\|^2 / 4$, where $X_j$ denotes the $j$-th column of $X$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 5:</b><br/>\n",
    "    Code cyclic proximal coordinate descent for sparse Logistic regression:\n",
    "</div>\n",
    "\n",
    "**WARNING**: the Lasso means linear regression (quadratic fitting term) with L1 penalty. Sparse logistic regression means logistic regression with L1 penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = simu(coefs, n_samples=1000, for_logreg=True)\n",
    "lambda_max = norm(X.T.dot(y), ord= np.inf) / 2.\n",
    "lamb = lambda_max / 20.  \n",
    "# much easier to parametrize lambda as a function of lambda_max than \n",
    "# to take random values like 0.1 in previous Labs\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    return 1. / (1. + np.exp(-t))\n",
    "\n",
    "\n",
    "def soft_thresh(x, u):\n",
    "    \"\"\"Soft thresholding of x at level u\"\"\"\n",
    "    return np.sign(x) * np.maximum(0., np.abs(x) - u)\n",
    "\n",
    "\n",
    "def cd_logreg(X, y, lamb, n_iter):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    Xw = X.dot(w)\n",
    "    \n",
    "    # TODO\n",
    "    # lips_const = \n",
    "    # END TODO\n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        for j in range(n_features):\n",
    "            old_w_j = w[j]\n",
    "            # TODO\n",
    "            # grad_j = \n",
    "            # w[j] = soft_thresh(1, 2)\n",
    "            \n",
    "            # if old_w_j != w[j]:\n",
    "                # Xw += \n",
    "            #END TODO\n",
    "            \n",
    "        all_objs[t] = np.log(1. + np.exp(-y * Xw)).sum() + lamb * norm(w, ord=1)\n",
    "    \n",
    "    return w, all_objs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Real data\n",
    "\n",
    "We will compare vanilla cyclic CD and ISTA to solve the Lasso on a real dataset, called _leukemia_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alice\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\datasets\\_openml.py:311: UserWarning: Multiple active versions of the dataset matching the name leukemia exist. Versions may be fundamentally different, returning version 1.\n",
      "  warn(\n",
      "c:\\Users\\alice\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_openml\n\u001b[1;32m----> 3\u001b[0m leuk \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_openml\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mleukemia\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masfortranarray(leuk\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(leuk\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\alice\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\alice\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\datasets\\_openml.py:1128\u001b[0m, in \u001b[0;36mfetch_openml\u001b[1;34m(name, version, data_id, data_home, target_column, cache, return_X_y, as_frame, n_retries, delay, parser, read_csv_kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# obtain the data\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m url \u001b[38;5;241m=\u001b[39m _DATA_FILE\u001b[38;5;241m.\u001b[39mformat(data_description[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 1128\u001b[0m bunch \u001b[38;5;241m=\u001b[39m \u001b[43m_download_data_to_bunch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mas_frame\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopenml_columns_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmd5_checksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_description\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmd5_checksum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_csv_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_csv_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_X_y:\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bunch\u001b[38;5;241m.\u001b[39mdata, bunch\u001b[38;5;241m.\u001b[39mtarget\n",
      "File \u001b[1;32mc:\\Users\\alice\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\datasets\\_openml.py:677\u001b[0m, in \u001b[0;36m_download_data_to_bunch\u001b[1;34m(url, sparse, data_home, as_frame, openml_columns_info, data_columns, target_columns, shape, md5_checksum, n_retries, delay, parser, read_csv_kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParserError\n\u001b[0;32m    675\u001b[0m     no_retry_exception \u001b[38;5;241m=\u001b[39m ParserError\n\u001b[1;32m--> 677\u001b[0m X, y, frame, categories \u001b[38;5;241m=\u001b[39m \u001b[43m_retry_with_clean_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_retry_exception\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_load_arff_response\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopenml_columns_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names_to_select\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_names_to_select\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmd5_checksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmd5_checksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_csv_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_csv_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Bunch(\n\u001b[0;32m    695\u001b[0m     data\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    696\u001b[0m     target\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    700\u001b[0m     target_names\u001b[38;5;241m=\u001b[39mtarget_columns,\n\u001b[0;32m    701\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\alice\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\datasets\\_openml.py:67\u001b[0m, in \u001b[0;36m_retry_with_clean_cache.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m URLError:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alice\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\datasets\\_openml.py:542\u001b[0m, in \u001b[0;36m_load_arff_response\u001b[1;34m(url, data_home, parser, output_type, openml_columns_info, feature_names_to_select, target_names_to_select, shape, md5_checksum, n_retries, delay, read_csv_kwargs)\u001b[0m\n\u001b[0;32m    532\u001b[0m arff_params: Dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    533\u001b[0m     parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[0;32m    534\u001b[0m     output_type\u001b[38;5;241m=\u001b[39moutput_type,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    539\u001b[0m     read_csv_kwargs\u001b[38;5;241m=\u001b[39mread_csv_kwargs \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[0;32m    540\u001b[0m )\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 542\u001b[0m     X, y, frame, categories \u001b[38;5;241m=\u001b[39m \u001b[43m_open_url_and_load_gzip_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marff_params\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parser \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\alice\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\datasets\\_openml.py:530\u001b[0m, in \u001b[0;36m_load_arff_response.<locals>._open_url_and_load_gzip_file\u001b[1;34m(url, data_home, n_retries, delay, arff_params)\u001b[0m\n\u001b[0;32m    528\u001b[0m gzip_file \u001b[38;5;241m=\u001b[39m _open_openml_url(url, data_home, n_retries\u001b[38;5;241m=\u001b[39mn_retries, delay\u001b[38;5;241m=\u001b[39mdelay)\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m closing(gzip_file):\n\u001b[1;32m--> 530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_arff_from_gzip_file(gzip_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marff_params)\n",
      "File \u001b[1;32mc:\\Users\\alice\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\datasets\\_arff_parser.py:520\u001b[0m, in \u001b[0;36mload_arff_from_gzip_file\u001b[1;34m(gzip_file, parser, output_type, openml_columns_info, feature_names_to_select, target_names_to_select, shape, read_csv_kwargs)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a compressed ARFF file using a given parser.\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;124;03m    `output_array_type == \"pandas\"`.\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parser \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliac-arff\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_liac_arff_parser\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgzip_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopenml_columns_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_names_to_select\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_names_to_select\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m parser \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pandas_arff_parser(\n\u001b[0;32m    530\u001b[0m         gzip_file,\n\u001b[0;32m    531\u001b[0m         output_type,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    535\u001b[0m         read_csv_kwargs,\n\u001b[0;32m    536\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\alice\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\datasets\\_arff_parser.py:223\u001b[0m, in \u001b[0;36m_liac_arff_parser\u001b[1;34m(gzip_file, output_arrays_type, openml_columns_info, feature_names_to_select, target_names_to_select, shape)\u001b[0m\n\u001b[0;32m    221\u001b[0m         dtypes[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 223\u001b[0m         dtypes[name] \u001b[38;5;241m=\u001b[39m \u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtypes\u001b[49m[name]\n\u001b[0;32m    224\u001b[0m frame \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mastype(dtypes)\n\u001b[0;32m    226\u001b[0m X, y \u001b[38;5;241m=\u001b[39m _post_process_frame(\n\u001b[0;32m    227\u001b[0m     frame, feature_names_to_select, target_names_to_select\n\u001b[0;32m    228\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\alice\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:6365\u001b[0m, in \u001b[0;36mNDFrame.dtypes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   6337\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   6338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdtypes\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   6339\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6340\u001b[0m \u001b[38;5;124;03m    Return the dtypes in the DataFrame.\u001b[39;00m\n\u001b[0;32m   6341\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6363\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[0;32m   6364\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6365\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced(data, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mobject_)\n",
      "File \u001b[1;32mc:\\Users\\alice\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:279\u001b[0m, in \u001b[0;36mBaseBlockManager.get_dtypes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dtypes\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mobject_]:\n\u001b[1;32m--> 279\u001b[0m     dtypes \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblknos)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "leuk = fetch_openml(\"leukemia\")\n",
    "\n",
    "X = np.asfortranarray(leuk.data)\n",
    "y = np.ones(leuk.target.shape)\n",
    "y[leuk.target == leuk.target[0]] = -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 7129)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "\n",
    "lambda_max_lasso = norm(X.T.dot(y), ord=np.inf)\n",
    "lambd = lambda_max_lasso / 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 6:</b> Code\n",
    "    <ul>\n",
    "        <li>a simple proximal gradient solver for the Lasso</li>\n",
    "        <li>a prox CD solver for the Lasso and compare them on this dataset.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**Remark:** Do the plots in terms of epochs, not updates (to be fair to CD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
