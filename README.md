# Optimization Algorithms for Machine Learning  

![Optimization wallpaper](optimization.gif)

This repository is a comprehensive collection of notebooks and materials exploring **optimization algorithms for machine learning**, covering both theoretical foundations and practical applications. It is structured as a course and lab-based journey, going from core optimization concepts to advanced algorithms tailored to modern machine learning problems.

---

## ğŸ—‚ Directory Structure

```
â”œâ”€â”€ cours                                     # Original course material on coordinate descent
â”‚Â Â  â”œâ”€â”€ 01_intro_optim_ml.pdf
â”‚Â Â  â”œâ”€â”€ 02_grad_desc.pdf
â”‚Â Â  â”œâ”€â”€ 03_MLinpractice.pdf
â”‚Â Â  â”œâ”€â”€ 04_nonsmooth.pdf
â”‚Â Â  â”œâ”€â”€ 05_stochastic.pdf
â”‚Â Â  â”œâ”€â”€ 06_constrained_std.pdf
â”‚Â Â  â”œâ”€â”€ 3_prox.pdf
â”‚Â Â  â”œâ”€â”€ ex_intro_gd.pdf
â”‚Â Â  â”œâ”€â”€ gradient_descent.ipynb
â”‚Â Â  â””â”€â”€ materials_cd
â”‚Â Â      â””â”€â”€ materials_cd                      # Clean version of coordinate descent course content
â”‚Â Â          â”œâ”€â”€ coordinate_descent.ipynb
â”‚Â Â          â”œâ”€â”€ Lab_cd.ipynb
â”‚Â Â          â”œâ”€â”€ notes_cd.pdf
â”‚Â Â          â””â”€â”€ slides_cd.pdf
â”œâ”€â”€ cours_coordinate_descent                     
â”‚Â Â  â””â”€â”€ materials_cd
â”‚Â Â      â”œâ”€â”€ coordinate_descent.ipynb
â”‚Â Â      â”œâ”€â”€ Lab_cd.ipynb
â”‚Â Â      â”œâ”€â”€ notes_cd.pdf
â”‚Â Â      â””â”€â”€ slides_cd.pdf
â”œâ”€â”€ fw_non_cvx_autodiff
â”‚Â Â  â”œâ”€â”€ lecture_fw_non_cvx_autodiff
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 07-line_search_interactive.ipynb
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 08-Line_search_method.ipynb
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ adaptive_lasso.ipynb
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ automatic_differentiation.ipynb
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ coordinate_descent_non_cvx.ipynb
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ frank_wolfe.ipynb
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ movies
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 0_l22_film.mp4
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 1_l1_film.mp4
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 2_enet_film.mp4
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 3_l0_film.mp4
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 4_sqrt_film.mp4
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 5_log_film.mp4
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 6_mcp_film.mp4
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ 7_scad_film.mp4
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ movies_prox.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ optim_utils.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ prox_and_pen_plotting.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ prox_collection.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ slides_autodiff.pdf
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ slides_fw.pdf
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ slides_linear_search.pdf
â”‚Â Â  â”‚Â Â  â””â”€â”€ slides_non_cvx.pdf
â”œâ”€â”€ gradient_descent.ipynb
â”œâ”€â”€ lab1_chen_bryan_and_devilder_alice.ipynb
â”œâ”€â”€ lab2_chen_bryan_and_devilder_alice.ipynb
â”œâ”€â”€ lab3_chen_bryan_and_devilder_alice.ipynb
â”œâ”€â”€ material_quasi_newton                       # Code utilities for quasi-Newton methods (e.g., L-BFGS)
â”‚Â Â  â””â”€â”€ material_quasi_newton
â”‚Â Â      â”œâ”€â”€ Newton_1d.ipynb
â”‚Â Â      â”œâ”€â”€ Newton_variable_metric.ipynb
â”‚Â Â      â”œâ”€â”€ notes_quasi_newton.pdf
â”‚Â Â      â”œâ”€â”€ optim_utils.py
â”‚Â Â      â”œâ”€â”€ slides_quasi_newton.pdf
â”‚Â Â      â””â”€â”€ tp_newton_chen_bryan_and_devilder_alice.ipynb
â”œâ”€â”€ optimization.gif
â”œâ”€â”€ project                                     # Notebooks and experiments applying optimization methods
â”‚Â Â  â”œâ”€â”€ NOxEmissions.csv
â”‚Â Â  â””â”€â”€ project_chen_bryan_and_devilder_alice.ipynb

```

---

## ğŸ“š Course Overview

The notebooks and materials are structured around the following themes:

### ğŸ”¹ 1. Introduction to Optimization in Data Science

- **1.1** Machine Learning Optimization Problems + Linear Algebra Recap  
- **1.2** Optimization Problem Properties (Convexity, Smoothness)

### ğŸ”¹ 2. Smooth Optimization

- **2.1** Gradient Descent & Convergence  
- **3.1** Quadratic Problems and Conjugate Gradient  
- **3.2** Line Search Techniques

### ğŸ”¹ 3. Non-smooth Optimization

- **4.1** Proximal Operators & Algorithms  
- **4.2** Lab: Lasso & Group Lasso

### ğŸ”¹ 4. Stochastic Optimization

- **5.1** SGD and Variance Reduction  
- **5.2** Lab: SGD for Logistic Regression

### ğŸ”¹ 5. Constrained Optimization

- **6.1** Linear Programming (LP), Quadratic Programming (QP), and Mixed Integer Programming

---

### ğŸ”¹ 6. Coordinate Descent (CD)

Located in `cours_coordinate_descent/materials_cd/`

- **1** Exact Coordinate Descent  
- **2** Coordinate Gradient Descent  
- **3** Proximal Coordinate Descent  
- **4** Applications to ML Estimators (e.g., Lasso, SVM)

---

### ğŸ”¹ 7. Newton & Quasi-Newton Methods

Located in `material_quasi_newton/`

- **8.1** Second-order optimization: Newtonâ€™s method  
- **L-BFGS**: Quasi-Newton method for large-scale optimization  
- Applications: â„“2 Logistic Regression, Conditional Random Fields

---

### ğŸ”¹ 8. Beyond Convex Optimization

Located in `fw_non_cvx_autodiff/`

- Nonconvex Regularization  
- Frank-Wolfe Algorithm  
- Difference of Convex (DC) Programming  
- Automatic Differentiation

---

## ğŸ¤– Data and ML Problem Applications

The notebooks also integrate optimization techniques in practical ML scenarios:

### ğŸ§  1. Data & Preprocessing

- Data visualization and feature analysis  
- Defining your ML problem

### ğŸ“Š 2. Unsupervised Learning

- Clustering  
- Generative Modeling  
- Dimensionality Reduction  

### ğŸ·ï¸ 3. Supervised Learning

- Linear Models  
- Decision Trees & Ensemble Methods  
- Bayesian Decision Theory  

### ğŸ“ˆ 4. Model Evaluation

- Validation & Performance Metrics  
- Hyperparameter Selection  
- Model Interpretation

---

## ğŸ” Notebooks and Code

Every `.ipynb` notebook in this repository contains:

- Interactive implementations of optimization algorithms  
- Visualizations of convergence and error behavior  
- Application to ML tasks like regression and classification  
- Insights into theoretical properties (smoothness, convexity, duality)

Some files also include useful utilities like:
- `optim_utils.py`: Common optimization routines for experiments.

---

## ğŸ§¹ Notes

- The `__MACOSX/` and hidden `._*` files are system-generated and can be safely ignored or deleted.
- Python `.py` and notebook files are tested with Python 3.8+ and standard ML libraries like NumPy, SciPy, scikit-learn, and Matplotlib.

---

## ğŸš€ Getting Started

1. Clone the repo:
   ```bash
   git clone https://github.com/yourusername/optimisation-algorithms.git
   cd optimisation-algorithms
   ```
2. Install dependencies:
   ```bash
   pip install -r requirements.txt  # if available
   ```
3. Launch notebooks:
   ```bash
   jupyter notebook
   ```

---

## ğŸ‘¨â€ğŸ”¬ Authors & Acknowledgements

Developed as part of a data science optimization course.  
Special thanks to contributors, researchers, and instructors whose materials shaped this work.

